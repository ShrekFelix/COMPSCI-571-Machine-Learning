{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_CSV(file_name):\n",
    "    tmp = np.loadtxt(file_name, dtype=np.str, delimiter=\",\")\n",
    "    features = tmp[1:,:-1].astype(np.float)# load features\n",
    "    labels = tmp[1:,-1].astype(np.float)# load labels\n",
    "    return features, labels # return ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_stump(X, Y, surrogate=False, dI_list=False):\n",
    "    '''\n",
    "    Returns index of the best split and best surrogate split.\n",
    "    Could also return index of best surrogate split and variable importance measures for each variable.\n",
    "    '''\n",
    "    output = [None for i in range(3)]\n",
    "    \n",
    "    \n",
    "    #find best split\n",
    "    dI = [] #impurity measure reduction, for a stump with only one split, it's same as importance measurement\n",
    "    for f in range(len(X[0])): #iterate through all features\n",
    "        clp=0 # child left-of-split positive\n",
    "        cln=0 #                     negative\n",
    "        crp=0 #       right-of-split\n",
    "        crn=0 #\n",
    "        for n in range(len(X)): #iterate through all points\n",
    "            if X[n][f] <= 0:\n",
    "                if Y[n] > 0:\n",
    "                    clp += 1\n",
    "                else:\n",
    "                    cln += 1\n",
    "            else:\n",
    "                if Y[n] > 0:\n",
    "                    crp += 1\n",
    "                else:\n",
    "                    crn += 1\n",
    "        #compute p for both left and right child\n",
    "        pl = clp/(clp+cln)\n",
    "        pr = crp/(crp+crn)\n",
    "        #use Gini index as impurity measurement\n",
    "        Il = 2*pl*(1-pl)\n",
    "        Ir = 2*pr*(1-pr)\n",
    "        I_after = ((clp+cln)*Il + (crp+crn)*Ir)/len(X) # impurity of each child on weight average\n",
    "        I_before = 2*(crn+cln)*(clp+crp)/(len(X)**2)\n",
    "        dI.append(I_before - I_after)\n",
    "    \n",
    "    idx_best = dI.index(max(dI)) # find maximum reduction in impurity measure\n",
    "    output[0] = idx_best\n",
    "    \n",
    "    if dI_list:\n",
    "        output[2] = dI\n",
    "    \n",
    "    #find best surrogate split\n",
    "    if surrogate:\n",
    "        if len(X[0]) == 1: # only one feature\n",
    "            output[1] = 'NA'\n",
    "        else:\n",
    "            #compute min(pL, pR)\n",
    "            pL = 0\n",
    "            pR = 0\n",
    "            for n in range(len(X)):\n",
    "                if X[n][idx_best] <= 0:\n",
    "                    pL += 1\n",
    "                else:\n",
    "                    pR += 1\n",
    "            m = min(pL/len(X), pR/len(X))\n",
    "            #compute lambda(Predictive Measure of Association)\n",
    "            PMA = []\n",
    "            for f in range(len(X[0])):\n",
    "                if f != idx_best: #exclude best split\n",
    "                    pLL = 0\n",
    "                    pRR = 0\n",
    "                    for n in range(len(X)):\n",
    "                        if X[n][idx_best] <= 0 and X[n][f] <=0:\n",
    "                            pLL += 1\n",
    "                        if X[n][idx_best] > 0 and X[n][f] > 0:\n",
    "                            pRR += 1\n",
    "                    PMA.append( (m - (1 - pLL/len(X) - pRR/len(X)))/m )\n",
    "            idx_surrogate = PMA.index(max(PMA)) #this index is without the column of best split\n",
    "            idx_surrogate = idx_surrogate if idx_surrogate<idx_best else idx_surrogate+1 #bring the column of best split back\n",
    "\n",
    "            output[1] = idx_surrogate\n",
    "    \n",
    "    \n",
    "    while None in output:\n",
    "        output.remove(None)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " [0.2703185497750237,\n",
       "  0.10556222981883362,\n",
       "  0.0005964678609062801,\n",
       "  0.0005550311283423759,\n",
       "  0.00011621491914448612]]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = load_CSV('train.csv')\n",
    "binary_stump(X, Y, surrogate=True, dI_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so best split is on X1, the best surrogate split is on X2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Report the variable importance measurements from Equations (2) and (3) for the tree based on the best split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using equation 2:\n",
    "\n",
    "Since a stump only has one split, the variable importance is numerically the same as its impurity reduction.\n",
    "\n",
    "$ Imp^T(X_1)= 0.2703185497750237$\n",
    "\n",
    "$ Imp^T(X_2)=0 $\n",
    "\n",
    "$ Imp^T(X_3)=0 $\n",
    "\n",
    "$ Imp^T(X_4)=0 $\n",
    "\n",
    "$ Imp^T(X_5)=0 $\n",
    "\n",
    "Using equation 3:\n",
    "\n",
    "$ Imp^T(X_1)=0.2703185497750237 $\n",
    "\n",
    "$ Imp^T(X_2)=0.10556222981883362 $\n",
    "\n",
    "$ Imp^T(X_3)=0 $\n",
    "\n",
    "$ Imp^T(X_4)=0 $\n",
    "\n",
    "$ Imp^T(X_5)=0 $\n",
    "\n",
    "X1 and X2 are more important than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Report the mean least-squares error of predictions on the test dataset of both decision stumps from part (a)(i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse_for_binary_stump(X, Y, n):\n",
    "    e = 0\n",
    "    for i in range(len(X)):\n",
    "        if X[i][n] != Y[i]:\n",
    "            e += 1\n",
    "    return e/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_CSV('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_for_binary_stump(X, Y, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mse for best split is 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_for_binary_stump(X, Y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mse for best split is 0.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Grow a random forest of decision stumps on the training dataset for K = 1, . . . , 5 randomly selected variables in each stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[193, 201, 213, 187, 206],\n",
       "  [406, 297, 100, 98, 99],\n",
       "  [595, 301, 39, 41, 24],\n",
       "  [792, 208, 0, 0, 0],\n",
       "  [1000, 0, 0, 0, 0]],\n",
       " [[0, 0, 0, 0, 0],\n",
       "  [0, 106, 286, 287, 321],\n",
       "  [0, 330, 234, 172, 264],\n",
       "  [0, 588, 138, 64, 210],\n",
       "  [0, 1000, 0, 0, 0]],\n",
       " [array([ 0.,  0.,  0.,  0.,  0.]),\n",
       "  array([ 0.10957607,  0.03130225,  0.00025041,  0.00029355,  0.00022937]),\n",
       "  array([  1.61073806e-01,   3.15645630e-02,   1.78685373e-04,\n",
       "           1.07255466e-04,   7.85373480e-05]),\n",
       "  array([ 0.21349149,  0.02200919,  0.        ,  0.        ,  0.        ]),\n",
       "  array([ 0.27067617,  0.        ,  0.        ,  0.        ,  0.        ])],\n",
       " [array([ 0.07142113,  0.04567689,  0.00379972, -0.00310813,  0.00105875]),\n",
       "  array([ 0.12399624,  0.06818917,  0.00288835, -0.00300282,  0.00079027]),\n",
       "  array([ 0.14219984,  0.06835988,  0.00096638, -0.00122951,  0.00027607]),\n",
       "  array([ 0.15629116,  0.04621133,  0.        ,  0.        ,  0.        ]),\n",
       "  array([ 0.13702304,  0.        ,  0.        ,  0.        ,  0.        ])])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = load_CSV('train.csv')\n",
    "#each column represents a feature\n",
    "#each row represents a k value\n",
    "best_matrix = []\n",
    "surrogate_matrix = []\n",
    "Imp5_matrix = []\n",
    "Imp6_matrix = []\n",
    "\n",
    "for k in range(1,6):\n",
    "    best_list = [0 for i in range(5)]\n",
    "    surrogate_list = [0 for i in range(5)]\n",
    "    \n",
    "    #preallocate space so its faster\n",
    "    #use ndarray to do numerics(compute mean and std) on M*5 matrix\n",
    "    Imp5_table = np.array([[0 for i in range(5)] for i in range(1000)], dtype=float)\n",
    "    Imp6_table = np.array([[0 for i in range(5)] for i in range(1000)], dtype=float)\n",
    "    for M in range(1000):\n",
    "        \n",
    "        c = np.random.choice(5,k,replace=False) #randomly pick k features\n",
    "        r = np.random.choice(len(X),int(0.8*len(X)),replace=True) #bootstrap observations\n",
    "\n",
    "        #for (i) count votes for best split and best surrogate split\n",
    "        idx_best, idx_surrogate, dI = binary_stump(X[np.ix_(r, c)], Y[r], surrogate=True, dI_list=True) #grow stump on training data\n",
    "        #the resulting index is relative to sampled index\n",
    "       \n",
    "        best_list[c[idx_best]] += 1 #count as a vote\n",
    "        if idx_surrogate != 'NA': #k=1 there's no surrogate split\n",
    "            surrogate_list[c[idx_surrogate]] += 1\n",
    "            \n",
    "        #for (ii) compute variable importance using equation 5\n",
    "            Imp5_table[M][c[idx_best]] = dI[idx_best]\n",
    "        \n",
    "        #for (ii) compute variable importance using equation 6\n",
    "        r_oob = list(set(range(len(X))) - set(r)) #remaining observations        \n",
    "        tree_t = binary_stump(X[np.ix_(r_oob, c)], Y[r_oob]) #use oob rows to build a tree\n",
    "        e1 = mse_for_binary_stump(X[np.ix_(r_oob, c)], Y[r_oob], tree_t) #error for OOB tree\n",
    "        for j in range(len(c)):            \n",
    "            X_permuted = X[np.ix_(r_oob, c)] #prepare a copy of oob's observations for permutation\n",
    "            idx_permuted = np.random.permutation(range(len(X_permuted))) #generate a new order            \n",
    "            for i in range(len(X_permuted)): #permutation\n",
    "                X_permuted[i][j] = X[np.ix_(r_oob, c)][idx_permuted[i]][j]\n",
    "            tree_permuted = binary_stump(X_permuted, Y[r_oob]) #permuted tree\n",
    "            e2 = mse_for_binary_stump(X_permuted, Y[r_oob], tree_permuted) #error for permuted OOB tree\n",
    "        \n",
    "            Imp6_table[M][c[j]] = e2 - e1\n",
    "            \n",
    "    best_matrix.append( best_list )\n",
    "    surrogate_matrix.append( surrogate_list )\n",
    "    Imp5_matrix.append(Imp5_table.mean(axis=0))\n",
    "    Imp6_matrix.append(Imp6_table.mean(axis=0))\n",
    "\n",
    "best_matrix, surrogate_matrix, Imp5_matrix, Imp6_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) How many times is each variable the best split? How many times is each variable the best surrogate split? Does this suggest any variable(s) are more important than the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to best_matrix and surrogate matrix\n",
    "\n",
    "X1 and X2 are more important than the others.\n",
    "\n",
    "The obviousness of a variable's importance grows when k increases, since k is picked randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Compute the variable importance measures in Equations (5) and (6).Does this suggest any variable(s) are more important than the others? Recall that when using Equation (2) to compute variable importance for decision stumps, the phenomenon of “masking” can hide the potential variable importance of some variables. When using Equations (5) and (6) to compute variable importance for random forests, can masking similarly hide the potential variable importance of some variables, or is the impact of masking lessened? (On the topic of masking, you do not need to provide a numerical answer, just a brief discussion of the role of masking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to Imp5_matrix and Imp6_matrix.\n",
    "\n",
    "X1,X2 are more important\n",
    "\n",
    "Compared to decision trees, decision forest tends to lesson the impact of masking. But when k=5, X1 is guaranteed to be selected, it masks potential important variable in this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Compute the mean least-squares loss on the test data using two methods. In the first method, use the majority vote of the stumps as the prediction and compute the loss. In the second method, find the predictions of each stump, compute least-squares loss on each, and average the results. Which method is correct for computing the prediction error of the random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first method, use the majority vote of the stumps as the prediction and compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = load_CSV('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49, 0.1, 0.1, 0.1, 0.1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1_list = [0 for i in range(5)] #for each k\n",
    "for k in range(5):\n",
    "    loss1_list[k] = mse_for_binary_stump(X, Y, best_matrix[k].index(max(best_matrix[k])))\n",
    "loss1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second method, find the predictions of each stump, compute least-squares loss on each, and average the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38173, 0.2722, 0.19418999999999997, 0.13536, 0.1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2_list = [0 for i in range(5)] #for each k\n",
    "for k in range(5):\n",
    "    for f in range(5):\n",
    "        loss2_list[k] += mse_for_binary_stump(X, Y, f)*best_matrix[k][f]/1000\n",
    "        \n",
    "loss2_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Grow a random forest of decision stumps with B = q × n bootstrap samples, for each of q ∈ {0.4, 0.5, 0.6, 0.7, 0.8}. Use K = 2 randomly selected variables in each stump (this is the closest to the default choice of √p ≈ 2.23) and M = 1000 stumps. In the first method, use the majority vote of the stumps as the prediction and compute the loss. In the second method, find the predictions of each stump, compute least-squares loss on each, and average the results. Which method is correct for computing the prediction error of the random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[355, 300, 110, 131, 104],\n",
       "  [419, 277, 113, 104, 87],\n",
       "  [371, 339, 102, 100, 88],\n",
       "  [406, 290, 97, 116, 91],\n",
       "  [391, 289, 108, 119, 93]],\n",
       " [[0, 92, 273, 320, 315],\n",
       "  [0, 114, 289, 279, 318],\n",
       "  [0, 94, 274, 301, 331],\n",
       "  [0, 89, 307, 301, 303],\n",
       "  [0, 90, 304, 268, 338]],\n",
       " [array([ 0.09623129,  0.03295723,  0.00058393,  0.00072858,  0.00047822]),\n",
       "  array([ 0.1128168 ,  0.02942892,  0.00049635,  0.0003476 ,  0.00030009]),\n",
       "  array([ 0.09973266,  0.03552601,  0.00035267,  0.00033933,  0.00021404]),\n",
       "  array([ 0.10952892,  0.03009467,  0.00028424,  0.00031097,  0.00021074]),\n",
       "  array([ 0.10566026,  0.03026668,  0.0003598 ,  0.00035735,  0.00021972])],\n",
       " [array([ 0.10891909,  0.06842165,  0.00265634, -0.00245961,  0.00114416]),\n",
       "  array([ 0.12703885,  0.06290584,  0.0028174 , -0.00357756,  0.0012235 ]),\n",
       "  array([ 0.11452729,  0.07746121,  0.00277856, -0.00281501,  0.00133799]),\n",
       "  array([ 0.12909   ,  0.06646873,  0.00264669, -0.00304943,  0.00122298]),\n",
       "  array([ 0.12179262,  0.06568933,  0.00288023, -0.00173842,  0.00160247])],\n",
       " [array([ 0.13162185,  0.05289593,  0.00228987,  0.00277833,  0.00201784]),\n",
       "  array([ 0.13434434,  0.04932863,  0.0019816 ,  0.00147042,  0.00155789]),\n",
       "  array([ 0.13092384,  0.05146434,  0.0014669 ,  0.00151577,  0.00101172]),\n",
       "  array([ 0.13354315,  0.04851647,  0.0011969 ,  0.00119499,  0.00091101]),\n",
       "  array([ 0.13274825,  0.04884276,  0.00134358,  0.00138695,  0.00098069])],\n",
       " [array([ 0.15936583,  0.10610256,  0.01588817,  0.016944  ,  0.01551524]),\n",
       "  array([ 0.16486364,  0.10335222,  0.01766521,  0.01684407,  0.01458228]),\n",
       "  array([ 0.16234997,  0.11028359,  0.01626456,  0.01830773,  0.01549785]),\n",
       "  array([ 0.16865144,  0.10668736,  0.01884654,  0.01913819,  0.0162295 ]),\n",
       "  array([ 0.16522068,  0.10598908,  0.01977724,  0.02017041,  0.01945683])])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = load_CSV('train.csv')\n",
    "#each column represents a feature\n",
    "#each row represents a k value\n",
    "best_matrix = []\n",
    "surrogate_matrix = []\n",
    "Imp5_matrix = []\n",
    "Imp5std_matrix = []\n",
    "Imp6_matrix = []\n",
    "Imp6std_matrix = []\n",
    "\n",
    "for q in [0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    best_list = [0 for i in range(5)]\n",
    "    surrogate_list = [0 for i in range(5)]\n",
    "\n",
    "    Imp5_table = np.array([[0 for i in range(5)] for i in range(1000)], dtype=float)\n",
    "    Imp6_table = np.array([[0 for i in range(5)] for i in range(1000)], dtype=float)\n",
    "    for M in range(1000):\n",
    "        \n",
    "        c = np.random.choice(5,2,replace=False) #randomly pick k features\n",
    "        r = np.random.choice(len(X),int(q*len(X)),replace=True) #bootstrap observations\n",
    "\n",
    "        #for (i) count votes for best split and best surrogate split\n",
    "        idx_best, idx_surrogate, dI = binary_stump(X[np.ix_(r, c)], Y[r], surrogate=True, dI_list=True) #grow stump on training data\n",
    "        #the resulting index is relative to sampled index\n",
    "       \n",
    "        best_list[c[idx_best]] += 1 #count as a vote\n",
    "        if idx_surrogate != 'NA': #k=1 there's no surrogate split\n",
    "            surrogate_list[c[idx_surrogate]] += 1\n",
    "            \n",
    "        #for (ii) compute variable importance using equation 5\n",
    "            Imp5_table[M][c[idx_best]] = dI[idx_best]\n",
    "        \n",
    "        #for (ii) compute variable importance using equation 6\n",
    "        r_oob = list(set(range(len(X))) - set(r)) #remaining observations        \n",
    "        tree_t = binary_stump(X[np.ix_(r_oob, c)], Y[r_oob]) #use oob rows to build a tree\n",
    "        e1 = mse_for_binary_stump(X[np.ix_(r_oob, c)], Y[r_oob], tree_t) #error for OOB tree\n",
    "        for j in range(len(c)):            \n",
    "            X_permuted = X[np.ix_(r_oob, c)] #prepare a copy of oob's observations for permutation\n",
    "            idx_permuted = np.random.permutation(range(len(X_permuted))) #generate a new order            \n",
    "            for i in range(len(X_permuted)): #permutation\n",
    "                X_permuted[i][j] = X[np.ix_(r_oob, c)][idx_permuted[i]][j]\n",
    "            tree_permuted = binary_stump(X_permuted, Y[r_oob]) #permuted tree\n",
    "            e2 = mse_for_binary_stump(X_permuted, Y[r_oob], tree_permuted) #error for permuted OOB tree\n",
    "        \n",
    "            Imp6_table[M][c[j]] = e2 - e1\n",
    "            \n",
    "    best_matrix.append( best_list )\n",
    "    surrogate_matrix.append( surrogate_list )\n",
    "    Imp5_matrix.append(Imp5_table.mean(axis=0))\n",
    "    Imp5std_matrix.append(Imp5_table.std(axis=0))\n",
    "    Imp6_matrix.append(Imp6_table.mean(axis=0))\n",
    "    Imp6std_matrix.append(Imp6_table.std(axis=0))\n",
    "\n",
    "best_matrix, surrogate_matrix, Imp5_matrix, Imp6_matrix, Imp5std_matrix, Imp6std_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Compute the variable importance measurements in Equations (5) and (6). Does this suggest any variable(s) are more important than the others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to Imp5_matrix and Imp6_matrix.\n",
    "\n",
    "It suggests X1,X2 are more important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Compute the standard deviation of the variable importance measurements in Equations (5) and (6). That is, instead of computing the mean over the M stumps in Equations (5) and (6), compute the sample standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to Imp5std_matrix and Imp6std_matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
