import numpy as np

def load_CSV(file_name):
    tmp = np.loadtxt(file_name, dtype=np.str, delimiter=",")
    features = tmp[1:,:-1].astype(np.float)# load features
    labels = tmp[1:,-1].astype(np.float)# load labels
    return features, labels # return ndarray

def binary_stump(X, Y, surrogate=False, dI_list=False):
    '''
    Returns index of the best split and best surrogate split.
    Could also return index of best surrogate split and variable importance measures for each variable.
    '''
    output = [None for i in range(3)]
    
    
    #find best split
    dI = [] #impurity measure reduction, for a stump with only one split, it's same as importance measurement
    for f in range(len(X[0])): #iterate through all features
        clp=0 # child left-of-split positive
        cln=0 #                     negative
        crp=0 #       right-of-split
        crn=0 #
        for n in range(len(X)): #iterate through all points
            if X[n][f] <= 0:
                if Y[n] > 0:
                    clp += 1
                else:
                    cln += 1
            else:
                if Y[n] > 0:
                    crp += 1
                else:
                    crn += 1
        #compute p for both left and right child
        pl = clp/(clp+cln)
        pr = crp/(crp+crn)
        #use Gini index as impurity measurement
        Il = 2*pl*(1-pl)
        Ir = 2*pr*(1-pr)
        I_after = ((clp+cln)*Il + (crp+crn)*Ir)/len(X) # impurity of each child on weight average
        I_before = 2*(crn+cln)*(clp+crp)/(len(X)**2)
        dI.append(I_before - I_after)
    
    idx_best = dI.index(max(dI)) # find maximum reduction in impurity measure
    output[0] = idx_best
    
    if dI_list:
        output[2] = dI
    
    #find best surrogate split
    if surrogate:
        if len(X[0]) == 1: # only one feature
            output[1] = 'NA'
        else:
            #compute min(pL, pR)
            pL = 0
            pR = 0
            for n in range(len(X)):
                if X[n][idx_best] <= 0:
                    pL += 1
                else:
                    pR += 1
            m = min(pL/len(X), pR/len(X))
            #compute lambda(Predictive Measure of Association)
            PMA = []
            for f in range(len(X[0])):
                if f != idx_best: #exclude best split
                    pLL = 0
                    pRR = 0
                    for n in range(len(X)):
                        if X[n][idx_best] <= 0 and X[n][f] <=0:
                            pLL += 1
                        if X[n][idx_best] > 0 and X[n][f] > 0:
                            pRR += 1
                    PMA.append( (m - (1 - pLL/len(X) - pRR/len(X)))/m )
            idx_surrogate = PMA.index(max(PMA)) #this index is without the column of best split
            idx_surrogate = idx_surrogate if idx_surrogate<idx_best else idx_surrogate+1 #bring the column of best split back

            output[1] = idx_surrogate
    
    
    while None in output:
        output.remove(None)
    return output



for k in range(1,6):
    best_list = [0 for i in range(5)]
    surrogate_list = [0 for i in range(5)]
    
    #preallocate space so its faster
    #use ndarray to do numerics(compute mean and std) on M*5 matrix
    Imp5_table = np.array([[0 for i in range(5)] for i in range(1000)], dtype=float)
    Imp6_table = np.array([[0 for i in range(5)] for i in range(1000)], dtype=float)
    for M in range(1000):
        
        c = np.random.choice(5,k,replace=False) #randomly pick k features
        r = np.random.choice(len(X),int(0.8*len(X)),replace=True) #bootstrap observations

        #for (i) count votes for best split and best surrogate split
        idx_best, idx_surrogate, dI = binary_stump(X[np.ix_(r, c)], Y[r], surrogate=True, dI_list=True) #grow stump on training data
        #the resulting index is relative to sampled index
       
        best_list[c[idx_best]] += 1 #count as a vote
        if idx_surrogate != 'NA': #k=1 there's no surrogate split
            surrogate_list[c[idx_surrogate]] += 1
            
        #for (ii) compute variable importance using equation 5
            Imp5_table[M][c[idx_best]] = dI[idx_best]
        
        #for (ii) compute variable importance using equation 6
        r_oob = list(set(range(len(X))) - set(r)) #remaining observations        
        tree_t = binary_stump(X[np.ix_(r_oob, c)], Y[r_oob]) #use oob rows to build a tree
        e1 = mse_for_binary_stump(X[np.ix_(r_oob, c)], Y[r_oob], tree_t) #error for OOB tree
        for j in range(len(c)):            
            X_permuted = X[np.ix_(r_oob, c)] #prepare a copy of oob's observations for permutation
            idx_permuted = np.random.permutation(range(len(X_permuted))) #generate a new order            
            for i in range(len(X_permuted)): #permutation
                X_permuted[i][j] = X[np.ix_(r_oob, c)][idx_permuted[i]][j]
            tree_permuted = binary_stump(X_permuted, Y[r_oob]) #permuted tree
            e2 = mse_for_binary_stump(X_permuted, Y[r_oob], tree_permuted) #error for permuted OOB tree
        
            Imp6_table[M][c[j]] = e2 - e1
            
    best_matrix.append( best_list )
    surrogate_matrix.append( surrogate_list )
    Imp5_matrix.append(Imp5_table.mean(axis=0))
    Imp6_matrix.append(Imp6_table.mean(axis=0))

